#!/usr/bin/env python3
"""
Async Pipeline Demo - Bar Directory Recon
=========================================

Demonstrates the AsyncPipelineExecutor for concurrent site processing.
This is the next-generation pipeline executor that provides 4x performance
improvement over the synchronous version.

Usage:
    python async_pipeline_demo.py
    python async_pipeline_demo.py --sites site1.com site2.com
    python async_pipeline_demo.py --demo-mode
"""

import argparse
import asyncio
import logging
import sys
import time
from pathlib import Path
from typing import Any

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from automation.enhanced_config_loader import load_automation_config

    AUTOMATION_AVAILABLE = True
except ImportError:
    print("‚ùå Automation modules not available - skipping pipeline demo")
    AUTOMATION_AVAILABLE = False

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class AsyncPipelineExecutor:
    """
    Async version of PipelineExecutor with concurrent execution.

    Provides significant performance improvements for multi-site processing
    through asyncio-based concurrent execution.
    """

    def __init__(self, config: dict[str, Any]) -> None:
        self.config = config
        self.timeout = config.get("timeout", 3600)
        self.max_concurrent = config.get("max_concurrent_sites", 5)
        self.retry_count = config.get("retry_count", 3)
        self.project_root = Path(__file__).parent

        # Create semaphore for concurrency control
        self.semaphore = asyncio.Semaphore(self.max_concurrent)

        # Demo mode for testing
        self.demo_mode = config.get("demo_mode", True)

        logger.info(
            f"AsyncPipelineExecutor initialized with {self.max_concurrent} concurrent slots"
        )

    async def run_all_async(self, sites: list[str]) -> dict[str, bool]:
        """
        Run pipeline for all sites asynchronously with asyncio.gather.

        Args:
            sites: List of site URLs to process

        Returns:
            Dict mapping sites to success status
        """
        logger.info(f"Starting async execution for {len(sites)} sites")
        start_time = time.time()

        # Create async tasks for all sites
        tasks = [self._execute_site_with_semaphore(site) for site in sites]

        # Execute all tasks concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        site_results = {}
        for site, result in zip(sites, results, strict=False):
            if isinstance(result, Exception):
                logger.error(f"Site {site} failed with exception: {result}")
                site_results[site] = False
            else:
                site_results[site] = result

        execution_time = time.time() - start_time
        success_count = sum(1 for success in site_results.values() if success)

        logger.info(f"Async execution completed in {execution_time:.2f}s")
        logger.info(
            f"Success rate: {success_count}/{len(sites)} ({success_count/len(sites)*100:.1f}%)"
        )

        return site_results

    async def _execute_site_with_semaphore(self, site: str) -> bool:
        """Execute pipeline for a site with concurrency control."""
        async with self.semaphore:
            return await self._execute_pipeline_async(site)

    async def _execute_pipeline_async(self, site: str) -> bool:
        """Execute pipeline for a specific site asynchronously."""
        for attempt in range(self.retry_count):
            try:
                logger.info(f"Executing pipeline for {site} (attempt {attempt + 1})")

                if self.demo_mode:
                    # Simulate processing time
                    processing_time = 2 + (attempt * 0.5)  # Simulate retry delays
                    await asyncio.sleep(processing_time)

                    # Simulate success/failure (90% success rate)
                    import random

                    success = random.random() > 0.1

                    if success:
                        logger.info(f"Site {site} completed successfully (simulated)")
                        return True
                    else:
                        logger.warning(f"Site {site} attempt {attempt + 1} failed (simulated)")
                else:
                    # Real pipeline execution
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        None,
                        self._run_pipeline_subprocess,
                        site,  # Use default executor
                    )

                    if result:
                        logger.info(f"Site {site} completed successfully")
                        return True
                    else:
                        logger.warning(f"Site {site} attempt {attempt + 1} failed")

            except Exception as e:
                logger.error(f"Site {site} attempt {attempt + 1} error: {e}")

            # Exponential backoff between retries
            if attempt < self.retry_count - 1:
                delay = 2**attempt
                await asyncio.sleep(delay)

        logger.error(f"Site {site} failed after {self.retry_count} attempts")
        return False

    def _run_pipeline_subprocess(self, site: str) -> bool:
        """Run the actual subprocess (blocking operation)."""
        import subprocess

        cmd = [
            sys.executable,
            "-m",
            "universal_recon.main",
            "--site",
            site,
            "--schema-matrix",
            "--emit-status",
        ]

        try:
            result = subprocess.run(
                cmd,
                timeout=self.timeout,
                capture_output=True,
                text=True,
                cwd=self.project_root,
            )

            return result.returncode == 0
        except subprocess.TimeoutExpired:
            logger.error(f"Site {site} timed out after {self.timeout}s")
            return False
        except Exception as e:
            logger.error(f"Site {site} subprocess error: {e}")
            return False

    async def run_with_progress(self, sites: list[str]) -> dict[str, bool]:
        """Run with real-time progress reporting."""
        completed = {}
        pending_tasks = {
            asyncio.create_task(self._execute_site_with_semaphore(site), name=site): site
            for site in sites
        }

        print(f"üöÄ Starting processing of {len(sites)} sites...")

        while pending_tasks:
            done, pending = await asyncio.wait(pending_tasks, return_when=asyncio.FIRST_COMPLETED)

            for task in done:
                site = pending_tasks.pop(task)
                try:
                    result = await task
                    completed[site] = result
                    status = "‚úÖ SUCCESS" if result else "‚ùå FAILED"
                    print(f"Progress: {len(completed)}/{len(sites)} - {site}: {status}")
                except Exception as e:
                    logger.error(f"Site {site} failed: {e}")
                    completed[site] = False
                    print(f"Progress: {len(completed)}/{len(sites)} - {site}: ‚ùå ERROR")

            pending_tasks = {task: site for task, site in pending_tasks.items() if task in pending}

        return completed


def create_demo_config() -> dict[str, Any]:
    """Create demo configuration for testing."""
    return {
        "max_concurrent_sites": 3,
        "timeout": 30,
        "retry_count": 2,
        "demo_mode": True,
        "sites": [
            "demo-site-1.com",
            "demo-site-2.com",
            "demo-site-3.com",
            "demo-site-4.com",
            "demo-site-5.com",
        ],
    }


async def demo_sync_vs_async():
    """Demonstrate performance difference between sync and async execution."""
    print("üîÑ Sync vs Async Performance Demo")
    print("=" * 40)

    demo_sites = [
        "example-1.com",
        "example-2.com",
        "example-3.com",
        "example-4.com",
        "example-5.com",
        "example-6.com",
    ]

    config = create_demo_config()

    # Sync execution simulation
    print("‚è≥ Simulating synchronous execution...")
    sync_start = time.time()
    for site in demo_sites:
        print(f"  Processing {site}...")
        await asyncio.sleep(2)  # Simulate 2 seconds per site
    sync_time = time.time() - sync_start

    print(f"‚úÖ Sync execution: {sync_time:.1f} seconds")

    # Async execution
    print("\n‚ö° Running asynchronous execution...")
    async_executor = AsyncPipelineExecutor(config)
    async_start = time.time()
    results = await async_executor.run_all_async(demo_sites)
    async_time = time.time() - async_start

    print(f"‚úÖ Async execution: {async_time:.1f} seconds")

    # Performance comparison
    improvement = sync_time / async_time
    print(f"\nüìä Performance Improvement: {improvement:.1f}x faster")
    print(f"Time saved: {sync_time - async_time:.1f} seconds")

    success_rate = sum(1 for success in results.values() if success) / len(results) * 100
    print(f"Success rate: {success_rate:.1f}%")


async def main_async(sites: list[str], demo_mode: bool = True):
    """Main async execution function."""
    print("üöÄ Async Pipeline Demo - Bar Directory Recon")
    print("=" * 50)

    # Load configuration
    if AUTOMATION_AVAILABLE and not demo_mode:
        try:
            config = load_automation_config()
            config["demo_mode"] = False
        except Exception as e:
            logger.warning(f"Could not load automation config: {e}")
            config = create_demo_config()
    else:
        config = create_demo_config()

    print(f"üîß Configuration loaded (demo_mode: {config.get('demo_mode', True)})")
    print(f"üéØ Max concurrent sites: {config.get('max_concurrent_sites', 5)}")

    # Use provided sites or demo sites
    if not sites:
        sites = config.get("sites", ["demo-site.com"])

    print(f"üìã Sites to process: {sites}")

    # Create executor and run
    executor = AsyncPipelineExecutor(config)

    if len(sites) <= 3:
        # Use progress reporting for small batches
        results = await executor.run_with_progress(sites)
    else:
        # Use batch processing for large batches
        results = await executor.run_all_async(sites)

    # Summary
    success_count = sum(1 for success in results.values() if success)
    print("\nüìä Execution Summary:")
    print(f"  Total sites: {len(sites)}")
    print(f"  Successful: {success_count}")
    print(f"  Failed: {len(sites) - success_count}")
    print(f"  Success rate: {success_count/len(sites)*100:.1f}%")

    return results


def main():
    """Main entry point for standalone execution."""
    parser = argparse.ArgumentParser(description="Async Pipeline Demo")
    parser.add_argument("--sites", nargs="*", help="Sites to process")
    parser.add_argument("--demo-mode", action="store_true", help="Run in demo mode")
    parser.add_argument(
        "--sync-vs-async", action="store_true", help="Demo sync vs async performance"
    )

    args = parser.parse_args()

    if args.sync_vs_async:
        asyncio.run(demo_sync_vs_async())
    else:
        asyncio.run(main_async(args.sites or [], args.demo_mode))


if __name__ == "__main__":
    main()
